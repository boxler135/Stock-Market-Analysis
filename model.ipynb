{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin with models, I'm gonna have to import all necessary libraries and load the cleaned data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Daily_Return</th>\n",
       "      <th>Rolling_Std</th>\n",
       "      <th>Lag_1</th>\n",
       "      <th>Lag_2</th>\n",
       "      <th>Lag_3</th>\n",
       "      <th>Lag_Return_1</th>\n",
       "      <th>Lag_Return_2</th>\n",
       "      <th>Lag_Return_3</th>\n",
       "      <th>Tomorrow</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-02-12 00:00:00-05:00</th>\n",
       "      <td>333.619995</td>\n",
       "      <td>333.619995</td>\n",
       "      <td>329.970001</td>\n",
       "      <td>330.079987</td>\n",
       "      <td>118390000</td>\n",
       "      <td>-0.010611</td>\n",
       "      <td>10.659240</td>\n",
       "      <td>333.619995</td>\n",
       "      <td>332.959991</td>\n",
       "      <td>333.750000</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>-0.002367</td>\n",
       "      <td>0.012407</td>\n",
       "      <td>331.019989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-13 00:00:00-05:00</th>\n",
       "      <td>330.079987</td>\n",
       "      <td>331.609985</td>\n",
       "      <td>327.920013</td>\n",
       "      <td>331.019989</td>\n",
       "      <td>144490000</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>9.876208</td>\n",
       "      <td>330.079987</td>\n",
       "      <td>333.619995</td>\n",
       "      <td>332.959991</td>\n",
       "      <td>-0.010611</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>-0.002367</td>\n",
       "      <td>332.010010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-14 00:00:00-05:00</th>\n",
       "      <td>331.019989</td>\n",
       "      <td>333.200012</td>\n",
       "      <td>330.640015</td>\n",
       "      <td>332.010010</td>\n",
       "      <td>138530000</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>8.987605</td>\n",
       "      <td>331.019989</td>\n",
       "      <td>330.079987</td>\n",
       "      <td>333.619995</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>-0.010611</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>334.890015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-15 00:00:00-05:00</th>\n",
       "      <td>332.010010</td>\n",
       "      <td>335.209991</td>\n",
       "      <td>331.609985</td>\n",
       "      <td>334.890015</td>\n",
       "      <td>174620000</td>\n",
       "      <td>0.008674</td>\n",
       "      <td>8.171342</td>\n",
       "      <td>332.010010</td>\n",
       "      <td>331.019989</td>\n",
       "      <td>330.079987</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>-0.010611</td>\n",
       "      <td>332.720001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-16 00:00:00-05:00</th>\n",
       "      <td>334.890015</td>\n",
       "      <td>335.640015</td>\n",
       "      <td>332.420013</td>\n",
       "      <td>332.720001</td>\n",
       "      <td>166840000</td>\n",
       "      <td>-0.006480</td>\n",
       "      <td>7.518505</td>\n",
       "      <td>334.890015</td>\n",
       "      <td>332.010010</td>\n",
       "      <td>331.019989</td>\n",
       "      <td>0.008674</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>327.989990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26 00:00:00-04:00</th>\n",
       "      <td>5433.669922</td>\n",
       "      <td>5488.319824</td>\n",
       "      <td>5430.700195</td>\n",
       "      <td>5459.100098</td>\n",
       "      <td>3638770000</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>70.624169</td>\n",
       "      <td>5399.220215</td>\n",
       "      <td>5427.129883</td>\n",
       "      <td>5555.740234</td>\n",
       "      <td>-0.005143</td>\n",
       "      <td>-0.023149</td>\n",
       "      <td>-0.001558</td>\n",
       "      <td>5463.540039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-29 00:00:00-04:00</th>\n",
       "      <td>5476.549805</td>\n",
       "      <td>5487.740234</td>\n",
       "      <td>5444.439941</td>\n",
       "      <td>5463.540039</td>\n",
       "      <td>3379970000</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>69.612330</td>\n",
       "      <td>5459.100098</td>\n",
       "      <td>5399.220215</td>\n",
       "      <td>5427.129883</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>-0.005143</td>\n",
       "      <td>-0.023149</td>\n",
       "      <td>5436.439941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-30 00:00:00-04:00</th>\n",
       "      <td>5478.729980</td>\n",
       "      <td>5489.459961</td>\n",
       "      <td>5401.700195</td>\n",
       "      <td>5436.439941</td>\n",
       "      <td>3777740000</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>69.410072</td>\n",
       "      <td>5463.540039</td>\n",
       "      <td>5459.100098</td>\n",
       "      <td>5399.220215</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>-0.005143</td>\n",
       "      <td>5522.299805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-31 00:00:00-04:00</th>\n",
       "      <td>5505.589844</td>\n",
       "      <td>5551.509766</td>\n",
       "      <td>5493.750000</td>\n",
       "      <td>5522.299805</td>\n",
       "      <td>4546910000</td>\n",
       "      <td>0.015793</td>\n",
       "      <td>68.886528</td>\n",
       "      <td>5436.439941</td>\n",
       "      <td>5463.540039</td>\n",
       "      <td>5459.100098</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>5446.680176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:00:00-04:00</th>\n",
       "      <td>5537.839844</td>\n",
       "      <td>5566.160156</td>\n",
       "      <td>5410.419922</td>\n",
       "      <td>5446.680176</td>\n",
       "      <td>4703620000</td>\n",
       "      <td>-0.013694</td>\n",
       "      <td>69.937897</td>\n",
       "      <td>5522.299805</td>\n",
       "      <td>5436.439941</td>\n",
       "      <td>5463.540039</td>\n",
       "      <td>0.015793</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>5346.560059</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8683 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Open         High          Low        Close  \\\n",
       "Date                                                                            \n",
       "1990-02-12 00:00:00-05:00   333.619995   333.619995   329.970001   330.079987   \n",
       "1990-02-13 00:00:00-05:00   330.079987   331.609985   327.920013   331.019989   \n",
       "1990-02-14 00:00:00-05:00   331.019989   333.200012   330.640015   332.010010   \n",
       "1990-02-15 00:00:00-05:00   332.010010   335.209991   331.609985   334.890015   \n",
       "1990-02-16 00:00:00-05:00   334.890015   335.640015   332.420013   332.720001   \n",
       "...                                ...          ...          ...          ...   \n",
       "2024-07-26 00:00:00-04:00  5433.669922  5488.319824  5430.700195  5459.100098   \n",
       "2024-07-29 00:00:00-04:00  5476.549805  5487.740234  5444.439941  5463.540039   \n",
       "2024-07-30 00:00:00-04:00  5478.729980  5489.459961  5401.700195  5436.439941   \n",
       "2024-07-31 00:00:00-04:00  5505.589844  5551.509766  5493.750000  5522.299805   \n",
       "2024-08-01 00:00:00-04:00  5537.839844  5566.160156  5410.419922  5446.680176   \n",
       "\n",
       "                               Volume  Daily_Return  Rolling_Std        Lag_1  \\\n",
       "Date                                                                            \n",
       "1990-02-12 00:00:00-05:00   118390000     -0.010611    10.659240   333.619995   \n",
       "1990-02-13 00:00:00-05:00   144490000      0.002848     9.876208   330.079987   \n",
       "1990-02-14 00:00:00-05:00   138530000      0.002991     8.987605   331.019989   \n",
       "1990-02-15 00:00:00-05:00   174620000      0.008674     8.171342   332.010010   \n",
       "1990-02-16 00:00:00-05:00   166840000     -0.006480     7.518505   334.890015   \n",
       "...                               ...           ...          ...          ...   \n",
       "2024-07-26 00:00:00-04:00  3638770000      0.011090    70.624169  5399.220215   \n",
       "2024-07-29 00:00:00-04:00  3379970000      0.000813    69.612330  5459.100098   \n",
       "2024-07-30 00:00:00-04:00  3777740000     -0.004960    69.410072  5463.540039   \n",
       "2024-07-31 00:00:00-04:00  4546910000      0.015793    68.886528  5436.439941   \n",
       "2024-08-01 00:00:00-04:00  4703620000     -0.013694    69.937897  5522.299805   \n",
       "\n",
       "                                 Lag_2        Lag_3  Lag_Return_1  \\\n",
       "Date                                                                \n",
       "1990-02-12 00:00:00-05:00   332.959991   333.750000      0.001982   \n",
       "1990-02-13 00:00:00-05:00   333.619995   332.959991     -0.010611   \n",
       "1990-02-14 00:00:00-05:00   330.079987   333.619995      0.002848   \n",
       "1990-02-15 00:00:00-05:00   331.019989   330.079987      0.002991   \n",
       "1990-02-16 00:00:00-05:00   332.010010   331.019989      0.008674   \n",
       "...                                ...          ...           ...   \n",
       "2024-07-26 00:00:00-04:00  5427.129883  5555.740234     -0.005143   \n",
       "2024-07-29 00:00:00-04:00  5399.220215  5427.129883      0.011090   \n",
       "2024-07-30 00:00:00-04:00  5459.100098  5399.220215      0.000813   \n",
       "2024-07-31 00:00:00-04:00  5463.540039  5459.100098     -0.004960   \n",
       "2024-08-01 00:00:00-04:00  5436.439941  5463.540039      0.015793   \n",
       "\n",
       "                           Lag_Return_2  Lag_Return_3     Tomorrow  Target  \n",
       "Date                                                                        \n",
       "1990-02-12 00:00:00-05:00     -0.002367      0.012407   331.019989       1  \n",
       "1990-02-13 00:00:00-05:00      0.001982     -0.002367   332.010010       1  \n",
       "1990-02-14 00:00:00-05:00     -0.010611      0.001982   334.890015       1  \n",
       "1990-02-15 00:00:00-05:00      0.002848     -0.010611   332.720001       0  \n",
       "1990-02-16 00:00:00-05:00      0.002991      0.002848   327.989990       0  \n",
       "...                                 ...           ...          ...     ...  \n",
       "2024-07-26 00:00:00-04:00     -0.023149     -0.001558  5463.540039       1  \n",
       "2024-07-29 00:00:00-04:00     -0.005143     -0.023149  5436.439941       0  \n",
       "2024-07-30 00:00:00-04:00      0.011090     -0.005143  5522.299805       1  \n",
       "2024-07-31 00:00:00-04:00      0.000813      0.011090  5446.680176       0  \n",
       "2024-08-01 00:00:00-04:00     -0.004960      0.000813  5346.560059       0  \n",
       "\n",
       "[8683 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load S&P 500 data\n",
    "sp500 = yf.Ticker(\"^GSPC\").history(period=\"max\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "sp500.drop(columns=['Dividends', 'Stock Splits'], inplace=True)\n",
    "\n",
    "# Filter out data prior to 1990\n",
    "sp500 = sp500.loc['1990-01-01':]\n",
    "\n",
    "# Handle missing values\n",
    "sp500.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
    "\n",
    "# Calculate Daily Return\n",
    "sp500['Daily_Return'] = sp500['Close'].pct_change()\n",
    "\n",
    "# Calculate Rolling Standard Deviation (30 days)\n",
    "sp500['Rolling_Std'] = sp500['Close'].rolling(window=30).std()\n",
    "\n",
    "# Calculate Lagged Prices (1, 2, 3 days)\n",
    "sp500['Lag_1'] = sp500['Close'].shift(1)\n",
    "sp500['Lag_2'] = sp500['Close'].shift(2)\n",
    "sp500['Lag_3'] = sp500['Close'].shift(3)\n",
    "\n",
    "# Calculate Lagged Returns (1, 2, 3 days)\n",
    "sp500['Lag_Return_1'] = sp500['Daily_Return'].shift(1)\n",
    "sp500['Lag_Return_2'] = sp500['Daily_Return'].shift(2)\n",
    "sp500['Lag_Return_3'] = sp500['Daily_Return'].shift(3)\n",
    "\n",
    "# Drop rows with NaN values (caused by shifting and rolling calculations)\n",
    "sp500.dropna(inplace=True)\n",
    "\n",
    "# Create the 'Tomorrow' column\n",
    "sp500['Tomorrow'] = sp500['Close'].shift(-1)\n",
    "\n",
    "# Create the 'Target' column\n",
    "sp500['Target'] = (sp500['Tomorrow'] > sp500['Close']).astype(int)\n",
    "\n",
    "# Drop the last row with NaN value in 'Tomorrow' column\n",
    "sp500.dropna(inplace=True)\n",
    "\n",
    "sp500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successful loading of dataset, we shall define the features and target variable, following that we will split the data into test and train set, with test being 20% and train being 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = sp500[['Daily_Return', 'Rolling_Std', 'Lag_1', 'Lag_2', 'Lag_3', 'Lag_Return_1', 'Lag_Return_2', 'Lag_Return_3']]\n",
    "target = sp500['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will standardise features then verify shape of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6946, 8)\n",
      "X_test shape: (1737, 8)\n",
      "y_train shape: (6946,)\n",
      "y_test shape: (1737,)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verify the shape of the datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "---\n",
    "\n",
    "### First model - Logistic Regression - Why Start with Logistic Regression?\n",
    "\n",
    "#### Simplicity and Interpretability\n",
    "Logistic Regression is one of the simplest and most interpretable classification algorithms. It provides clear insights into the relationships between the features and the target variable through the learned coefficients.\n",
    "\n",
    "#### Baseline Performance\n",
    "Starting with Logistic Regression allows us to establish a baseline performance. Since it is a relatively simple model, it helps us understand the basic patterns in the data before moving on to more complex models.\n",
    "\n",
    "#### Efficiency\n",
    "Logistic Regression is computationally efficient and can be trained quickly even on large datasets. This makes it an ideal choice for an initial model, allowing us to quickly iterate and evaluate results.\n",
    "\n",
    "#### Robustness\n",
    "Logistic Regression performs well on linearly separable data and is less prone to overfitting compared to more complex models. It provides a robust starting point for classification tasks.\n",
    "\n",
    "#### Handling Binary Classification\n",
    "Our task is a binary classification problem (predicting whether the stock price will increase or decrease the next day). Logistic Regression is specifically designed for binary classification and provides probability estimates, which can be useful for decision-making.\n",
    "\n",
    "By starting with Logistic Regression, we can gain initial insights into our dataset, establish a baseline, and use these results to guide further model selection and tuning.\n",
    "\n",
    "---\n",
    "\n",
    "We will initialise and train the logistic regression mode, then make predictions on the test set. From these predictions, I will use Area Under Curve (AUC) of ROC and accuracy score, as well as obtain the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5198618307426598\n",
      "ROC-AUC: 0.5010026134347545\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.04      0.07       833\n",
      "           1       0.52      0.96      0.68       904\n",
      "\n",
      "    accuracy                           0.52      1737\n",
      "   macro avg       0.51      0.50      0.37      1737\n",
      "weighted avg       0.51      0.52      0.39      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Evaluation\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "**Accuracy**: 0.52\n",
    "- The accuracy of the model is 51.98%, which indicates that the model is slightly better than random guessing.\n",
    "\n",
    "**ROC-AUC**: 0.50\n",
    "- The ROC-AUC score is 0.50, which suggests that the model has no discriminatory power and is equivalent to random guessing.\n",
    "\n",
    "### Analysis of Classification Report\n",
    "\n",
    "- **Class 0 (Decrease in stock price)**:\n",
    "  - Precision: 0.49\n",
    "  - Recall: 0.04\n",
    "  - F1-Score: 0.07\n",
    "  - The model performs poorly in identifying days where the stock price decreases. The recall is particularly low, indicating that the model fails to capture most of the instances of class 0.\n",
    "\n",
    "- **Class 1 (Increase in stock price)**:\n",
    "  - Precision: 0.52\n",
    "  - Recall: 0.96\n",
    "  - F1-Score: 0.68\n",
    "  - The model performs reasonably well in identifying days where the stock price increases, with a high recall but moderate precision. The high recall indicates that the model correctly identifies most instances of class 1, but the precision indicates that there are still many false positives.\n",
    "\n",
    "- **Overall Performance**:\n",
    "  - The model has a high recall for class 1 but fails to identify class 0 instances effectively. The overall performance metrics suggest that the model is not reliable enough for practical use.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The current Logistic Regression model shows a significant imbalance in performance between the two classes, with very poor detection of class 0 (decrease in stock price). Given these results, the model is not good enough for reliable predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "##### Following this, I will need a model that can have better performance, using these criterias to choose one:\n",
    "\n",
    "1. **Model Complexity**:\n",
    "   - Use more complex models such as Decision Trees, Random Forests, or Gradient Boosting Machines, which can capture non-linear relationships and interactions between features.\n",
    "\n",
    "2. **Handling Class Imbalance**:\n",
    "   - Implement techniques to handle class imbalance, such as oversampling the minority class (class 0), undersampling the majority class (class 1), or using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Explore additional feature engineering to create more informative features that could improve model performance.\n",
    "\n",
    "4. **Hyperparameter Tuning**:\n",
    "   - Perform hyperparameter tuning to optimize the performance of the chosen models.\n",
    "\n",
    "### Chosen Model: Random Forest\n",
    "\n",
    "**Reasoning for choosing this next model**:\n",
    "- **Robustness**: Random Forests are robust to overfitting and can handle non-linear relationships well.\n",
    "- **Feature Importance**: They provide insights into feature importance, which can be useful for further feature engineering.\n",
    "- **Performance**: They generally perform well on classification tasks, especially with imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import lib for Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4881980426021877\n",
      "ROC-AUC: 0.4827922319370226\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.35      0.40       833\n",
      "           1       0.51      0.62      0.56       904\n",
      "\n",
      "    accuracy                           0.49      1737\n",
      "   macro avg       0.48      0.48      0.48      1737\n",
      "weighted avg       0.48      0.49      0.48      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model Evaluation\n",
    "\n",
    "### Analysis\n",
    "\n",
    "- Both accuracy and ROC_AUC of Random Forest Classifier are slightly lower than those of Logistic Regression.\n",
    "- Same goes for classification report.\n",
    "- Only things that improved in Random Forest Classifier was in identifying the days where the stock price decreases (recall improved from 0.04 to 0.35, F1 score improved from 0.07 to 0.40, which is a huge improvement).\n",
    "\n",
    "### Conclusion and Next Steps\n",
    "\n",
    "The Random Forest model shows a mixed performance. While it improves the detection of class 0 (decrease in stock price), it performs worse in detecting class 1 (increase in stock price) compared to the Logistic Regression model. The overall performance metrics suggest that neither model is sufficiently reliable for practical use.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Model Complexity**:\n",
    "   - Explore other models such as Gradient Boosting Machines (GBM) or XGBoost, which might better capture the patterns in the data.\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Perform hyperparameter tuning for both Logistic Regression and Random Forest models to optimize their performance.\n",
    "\n",
    "3. **Handling Class Imbalance**:\n",
    "   - Implement techniques to handle class imbalance, such as oversampling, undersampling, or using class weights in the model.\n",
    "\n",
    "4. **Feature Engineering**:\n",
    "   - Further explore and create additional features that might help improve model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Model: Gradient Boosting Machines (GBM)\n",
    "\n",
    "**Reasoning**:\n",
    "- **Performance**: GBMs often perform well in a variety of tasks, including classification, and can handle complex relationships in the data.\n",
    "- **Handling Imbalance**: GBMs can handle class imbalance more effectively with techniques like custom loss functions and class weights.\n",
    "- **Fine-Tuning**: They offer many hyperparameters that can be fine-tuned to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import lib needed\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize and train the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5135290731145653\n",
      "ROC-AUC: 0.4996328177288614\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.16      0.24       833\n",
      "           1       0.52      0.84      0.64       904\n",
      "\n",
      "    accuracy                           0.51      1737\n",
      "   macro avg       0.50      0.50      0.44      1737\n",
      "weighted avg       0.50      0.51      0.45      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_gb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine (GBM) Model Evaluation\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "**Accuracy**: 0.51\n",
    "- The accuracy of the Gradient Boosting model is 51.35%, which is slightly higher than the Random Forest model's accuracy of 48.82% and similar to the Logistic Regression model's accuracy of 51.98%.\n",
    "\n",
    "**ROC-AUC**: 0.50\n",
    "- The ROC-AUC score is 0.50, which is comparable to the Logistic Regression model's ROC-AUC of 0.50 and higher than the Random Forest model's ROC-AUC of 0.48. This suggests that the Gradient Boosting model has similar discriminatory power as the Logistic Regression model.\n",
    "\n",
    "### Analysis of Classification Report\n",
    "\n",
    "- **Class 0 (Decrease in stock price)**:\n",
    "  - Precision: 0.48\n",
    "  - Recall: 0.16\n",
    "  - F1-Score: 0.24\n",
    "  - The Gradient Boosting model performs better in identifying class 0 instances compared to the Random Forest model (Recall improved from 0.35 to 0.16), but still fails to capture a significant portion of the class 0 instances.\n",
    "\n",
    "- **Class 1 (Increase in stock price)**:\n",
    "  - Precision: 0.52\n",
    "  - Recall: 0.84\n",
    "  - F1-Score: 0.64\n",
    "  - The performance of the Gradient Boosting model for class 1 is better than the Random Forest model (Recall improved from 0.62 to 0.84, F1-Score improved from 0.56 to 0.64) and comparable to the Logistic Regression model.\n",
    "\n",
    "- **Overall Performance**:\n",
    "  - The Gradient Boosting model shows an improvement in overall accuracy and ROC-AUC compared to the Random Forest model. However, it still struggles with detecting class 0 instances effectively. The recall for class 0 remains low, indicating that the model misses many days where the stock price decreases.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The Gradient Boosting model shows slight improvements in overall accuracy and ROC-AUC compared to the Random Forest model. However, it still faces challenges in effectively detecting decreases in stock prices (class 0). The performance metrics suggest that further improvements are needed to achieve reliable predictions.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Hyperparameter Tuning**:\n",
    "   - Perform hyperparameter tuning for the Gradient Boosting model to optimize its performance. Grid search or random search can be used to find the best hyperparameters.\n",
    "\n",
    "2. **Handling Class Imbalance**:\n",
    "   - Continue to address class imbalance through techniques like oversampling, undersampling, or using class weights.\n",
    "\n",
    "3. **Ensemble Methods**:\n",
    "   - Consider using ensemble methods that combine multiple models to improve performance. Techniques such as stacking or blending can leverage the strengths of different models.\n",
    "\n",
    "4. **Advanced Models**:\n",
    "   - Explore advanced models such as XGBoost or LightGBM, which are variations of gradient boosting and often provide better performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Rationale for Hyperparameter Tuning for Gradient Boosting\n",
    "\n",
    "### Current Model Performance\n",
    "- The Gradient Boosting model shows a slight improvement over the Random Forest model in terms of accuracy (51.35%) and ROC-AUC (0.50). \n",
    "- However, the model still struggles with effectively detecting decreases in stock prices (class 0), with a recall of only 0.16 for class 0.\n",
    "\n",
    "### Why Hyperparameter Tuning?\n",
    "1. **Potential for Improvement**:\n",
    "   - Gradient Boosting models have several hyperparameters that can significantly influence performance, such as the number of estimators, learning rate, and maximum depth of trees.\n",
    "   - Hyperparameter tuning can optimize these settings, potentially leading to substantial performance improvements without changing the underlying model.\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - Conducting hyperparameter tuning on the existing Gradient Boosting model is a more efficient approach than immediately switching to a new model. It allows us to leverage the current model's framework and possibly achieve satisfactory performance with the right parameters.\n",
    "\n",
    "3. **Baseline Understanding**:\n",
    "   - By fine-tuning the Gradient Boosting model, we establish a strong baseline performance. This helps in better understanding the potential and limitations of this model before exploring more complex models.\n",
    "   - If hyperparameter tuning yields significant improvements, it validates the use of Gradient Boosting and may reduce the need to switch to another model.\n",
    "\n",
    "### Why Not Advanced Models (XGBoost, LightGBM) Immediately?\n",
    "1. **Incremental Approach**:\n",
    "   - Adopting an incremental approach allows for systematic evaluation and optimization. Starting with hyperparameter tuning ensures we have exhausted the potential of simpler models before moving to more complex ones.\n",
    "\n",
    "2. **Resource Management**:\n",
    "   - Hyperparameter tuning on the existing model is less resource-intensive than training and tuning entirely new models like XGBoost or LightGBM.\n",
    "   - This step ensures efficient use of computational resources and time.\n",
    "\n",
    "3. **Model Understanding**:\n",
    "   - Gaining a thorough understanding of how well the Gradient Boosting model can perform with optimized parameters provides valuable insights.\n",
    "   - This understanding serves as a foundation for further model selection and tuning, should advanced models be required later.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps: Hyperparameter Tuning\n",
    "\n",
    "Given these considerations, we will proceed with hyperparameter tuning for the Gradient Boosting model. This step aims to optimize the current model's performance and determine if it can meet our predictive needs with the right parameter settings. If the tuned Gradient Boosting model still fails to provide satisfactory results, we will then consider exploring more advanced models like XGBoost or LightGBM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42),\n",
    "                           param_grid=param_grid, \n",
    "                           cv=3,                  # 3-fold cross-validation\n",
    "                           scoring='roc_auc',     # Evaluate performance using ROC-AUC\n",
    "                           n_jobs=-1)             # Use all available cores to speed up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingClassifier(random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.2],\n",
       "                         &#x27;max_depth&#x27;: [3, 4, 5],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingClassifier(random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.2],\n",
       "                         &#x27;max_depth&#x27;: [3, 4, 5],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingClassifier(random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n",
       "                         'max_depth': [3, 4, 5],\n",
       "                         'n_estimators': [100, 200, 300]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model (train with all combinations)\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_gb_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_best_gb = best_gb_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100}\n",
      "Accuracy: 0.5169833045480714\n",
      "ROC-AUC: 0.5102113854391315\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.34      0.41       833\n",
      "           1       0.53      0.68      0.59       904\n",
      "\n",
      "    accuracy                           0.52      1737\n",
      "   macro avg       0.51      0.51      0.50      1737\n",
      "weighted avg       0.51      0.52      0.50      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best_gb))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_best_gb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_best_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV Results for Gradient Boosting\n",
    "\n",
    "**Best Parameters**: `{'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100}`\n",
    "- These are the optimal hyperparameters found by GridSearchCV:\n",
    "  - **learning_rate**: 0.2\n",
    "  - **max_depth**: 5\n",
    "  - **n_estimators**: 100\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "**Accuracy**: 0.52\n",
    "- The accuracy of the tuned Gradient Boosting model is 51.70%, which is a slight improvement over the untuned Gradient Boosting model (51.35%).\n",
    "\n",
    "**ROC-AUC**: 0.51\n",
    "- The ROC-AUC score is 0.51, which is a slight improvement over the untuned Gradient Boosting model (0.50).\n",
    "\n",
    "### Analysis of classification report\n",
    "\n",
    "- **Class 0 (Decrease in stock price)**:\n",
    "  - Precision: 0.49\n",
    "  - Recall: 0.34\n",
    "  - F1-Score: 0.41\n",
    "  - The recall for class 0 has improved from 0.16 (untuned) to 0.34 (tuned), indicating better detection of days where the stock price decreases.\n",
    "\n",
    "- **Class 1 (Increase in stock price)**:\n",
    "  - Precision: 0.53\n",
    "  - Recall: 0.68\n",
    "  - F1-Score: 0.59\n",
    "  - The performance for class 1 is slightly lower compared to the untuned model (Recall decreased from 0.84 to 0.68).\n",
    "\n",
    "- **Overall Performance**:\n",
    "  - The overall accuracy and ROC-AUC scores have improved slightly. The macro average and weighted average F1-scores are now 0.50 and 0.50, respectively, suggesting a more balanced performance between the two classes.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The hyperparameter tuning for the Gradient Boosting model has resulted in slight improvements in accuracy and ROC-AUC. The model's ability to detect decreases in stock prices (class 0) has improved significantly, as evidenced by the increase in recall from 0.16 to 0.34. However, the performance for class 1 (increase in stock price) has slightly decreased. \n",
    "\n",
    "Overall, the tuned Gradient Boosting model shows better balance between the classes and a slight overall performance improvement. Further enhancements could involve additional feature engineering, exploring advanced models like XGBoost or LightGBM, or implementing ensemble methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Selection: Gradient Boosting Machine (GBM)\n",
    "\n",
    "### Conclusion\n",
    "After evaluating various models and performing hyperparameter tuning, we have chosen the Gradient Boosting Machine (GBM) as our final model. The tuned GBM model shows slight improvements in accuracy, ROC-AUC, and recall for class 0, resulting in a more balanced performance between the two classes.\n",
    "\n",
    "### Reasons for Choosing GBM\n",
    "1. **Improved Performance**: Hyperparameter tuning has led to better overall performance metrics.\n",
    "2. **Balanced Performance**: The model now demonstrates a more balanced detection of both classes.\n",
    "3. **Efficiency**: GBM provides satisfactory results without the need for switching to more complex models at this stage.\n",
    "\n",
    "### Next Steps\n",
    "1. **Document the Model**: Record the hyperparameters, performance metrics, and any relevant observations.\n",
    "2. **Feature Importance**: Analyze and document the importance of each feature in the model.\n",
    "3. **Model Deployment**: Prepare the model for deployment or application in the intended use case.\n",
    "\n",
    "### Optional Future Work\n",
    "If further improvements are desired, we can consider exploring advanced models like XGBoost or LightGBM, implementing ensemble methods, or conducting additional hyperparameter tuning, but for this project I will stick to tuned GBM.\n",
    "\n",
    "By choosing GBM, we ensure a robust and efficient model while keeping the door open for future enhancements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get feature importances\n",
    "feature_importances = best_gb_model.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance in Gradient Boosting Model')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "The feature importance analysis helps us understand which features contribute the most to the model's predictions. The plot below shows the importance of each feature in the Gradient Boosting model.\n",
    "\n",
    "### Key Findings\n",
    "- **`Lag_Return_3`**: The most important feature in predicting the target variable. This suggests that the return three days ago has a significant impact on the prediction.\n",
    "- **`Rolling_Std`**: The second most important feature, indicating that the 30-day rolling standard deviation is crucial for the model. This measure of volatility plays a key role in the prediction.\n",
    "- **`Lag_Return_2`**: Another important feature, highlighting that the return two days ago also significantly influences the model's predictions.\n",
    "- **`Daily_Return`**: The daily return is also a major factor, emphasizing the importance of immediate past performance in the prediction.\n",
    "- **`Lag_Return_1`**: The return one day ago is important but less so compared to the returns from two and three days ago.\n",
    "- **`Lag_3`, `Lag_2`, `Lag_1`**: The actual lagged prices for the past three days are less important than the lagged returns but still contribute to the model's predictions.\n",
    "\n",
    "### Interpretation\n",
    "- The model places more importance on lagged returns and volatility measures rather than just the raw lagged prices. This indicates that how the stock price changes over time (returns) and its volatility are more predictive of future movements than the absolute price levels of the past few days.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Interpret the Results**: Use the feature importance analysis to interpret the model's behavior and understand which features drive predictions.\n",
    "2. **Refine Features**: Consider refining or engineering additional features based on this analysis to further improve model performance.\n",
    "3. **Document Findings**: Record these findings in your documentation to provide insights into the model's decision-making process.\n",
    "\n",
    "By analyzing feature importance, we gain valuable insights into the factors driving our model's predictions, which can guide further improvements and ensure more informed decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the Model's Behavior\n",
    "\n",
    "Understanding which features drive the model's predictions can provide insights into how the model operates and why it makes certain predictions. Here are some points to consider for interpreting the model's behavior based on the feature importance analysis:\n",
    "\n",
    "##### Lagged Returns:\n",
    "`Lag_Return_3`, `Lag_Return_2`, `Lag_Return_1`: These features represent the returns from the previous three days. Their high importance suggests that the model heavily relies on recent past performance to predict future stock movements. The returns from three days ago have the highest importance, indicating that more distant past performance might provide more stable signals compared to immediate past performance.\n",
    "\n",
    "##### Volatility:\n",
    "`Rolling_Std`: The 30-day rolling standard deviation measures the volatility of the stock price. Its high importance indicates that the model considers periods of high volatility as significant indicators for predicting future price movements. This makes sense as high volatility often precedes significant price changes.\n",
    "\n",
    "##### Daily Return:\n",
    "Daily_Return: This feature captures the return from the previous day. Its significance underscores the model's focus on immediate past performance. It shows that daily market sentiment and movements have a strong impact on predictions.\n",
    "\n",
    "##### Lagged Prices:\n",
    "`Lag_3`, `Lag_2`, `Lag_1`: While these features are less important than lagged returns and volatility, they still contribute to the model. This suggests that absolute price levels from the past few days provide some useful information, albeit not as much as the returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we will move on to refining features to improve the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lagged features for the past 4 to 7 days\n",
    "for i in range(4, 8):\n",
    "    sp500[f'Lag_{i}'] = sp500['Close'].shift(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating rolling mean and rolling median features\n",
    "sp500['Rolling_Mean'] = sp500['Close'].rolling(window=30).mean()\n",
    "sp500['Rolling_Median'] = sp500['Close'].rolling(window=30).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating technical indicators\n",
    "sp500['MA50'] = sp500['Close'].rolling(window=50).mean()\n",
    "sp500['MA200'] = sp500['Close'].rolling(window=200).mean()\n",
    "\n",
    "# Relative Strength Index (RSI)\n",
    "delta = sp500['Close'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "rs = gain / loss\n",
    "sp500['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Moving Average Convergence Divergence (MACD)\n",
    "ema12 = sp500['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema26 = sp500['Close'].ewm(span=26, adjust=False).mean()\n",
    "sp500['MACD'] = ema12 - ema26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upon engineering these new features, next step is to retrain the GBM model, hyperparameter tune it, and re evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values created by lagging and rolling calculations\n",
    "sp500 = sp500.dropna()\n",
    "\n",
    "# Define the feature set and target variable\n",
    "features = ['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5', 'Lag_6', 'Lag_7', \n",
    "            'Lag_Return_1', 'Lag_Return_2', 'Lag_Return_3', 'Daily_Return', \n",
    "            'Rolling_Std', 'Rolling_Mean', 'Rolling_Median', 'MA50', 'MA200', 'RSI', 'MACD']\n",
    "X = sp500[features]\n",
    "y = sp500['Target']\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Accuracy: 0.516205067766647\n",
      "ROC-AUC: 0.49749947876458755\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.34      0.38       748\n",
      "           1       0.56      0.66      0.60       949\n",
      "\n",
      "    accuracy                           0.52      1697\n",
      "   macro avg       0.50      0.50      0.49      1697\n",
      "weighted avg       0.50      0.52      0.51      1697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42), param_grid=param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_best_gb = best_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best_gb))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_best_gb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_best_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Results for Gradient Boosting Machine (GBM)\n",
    "\n",
    "### Best Parameters\n",
    "- **learning_rate**: 0.1\n",
    "- **max_depth**: 5\n",
    "- **n_estimators**: 200\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "**Accuracy**: 0.52\n",
    "- The accuracy of the tuned Gradient Boosting model is 51.62%, which is slightly higher than the initial GBM model.\n",
    "\n",
    "**ROC-AUC**: 0.50\n",
    "- The ROC-AUC score is 0.50, which is similar to the initial GBM model.\n",
    "\n",
    "### Analysis of classification report\n",
    "\n",
    "- **Class 0 (Decrease in stock price)**:\n",
    "  - Precision: 0.44\n",
    "  - Recall: 0.34\n",
    "  - F1-Score: 0.38\n",
    "  - The recall for class 0 has slightly improved compared to previous models, indicating better detection of days where the stock price decreases.\n",
    "\n",
    "- **Class 1 (Increase in stock price)**:\n",
    "  - Precision: 0.56\n",
    "  - Recall: 0.66\n",
    "  - F1-Score: 0.60\n",
    "  - The performance for class 1 shows a balanced precision and recall, which is slightly improved over the previous models.\n",
    "\n",
    "- **Overall Performance**:\n",
    "  - The overall accuracy and ROC-AUC scores have improved slightly. The macro average and weighted average F1-scores are now 0.49 and 0.51, respectively, indicating a more balanced performance between the two classes.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The hyperparameter tuning for the Gradient Boosting model has resulted in slight improvements in accuracy, ROC-AUC, and recall for class 0. The model's ability to detect decreases in stock prices (class 0) has improved, and it continues to perform well for class 1 (increase in stock price)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "The feature importance analysis helps us understand which features contribute the most to the model's predictions. The plot below shows the importance of each feature in the Gradient Boosting model.\n",
    "\n",
    "### Key Findings\n",
    "- **`Lag_Return_3`**: The most important feature in predicting the target variable. This suggests that the return three days ago has a significant impact on the prediction.\n",
    "- **`Rolling_Std`**: The second most important feature, indicating that the 30-day rolling standard deviation is crucial for the model. This measure of volatility plays a key role in the prediction.\n",
    "- **`Lag_Return_2`**: Another important feature, highlighting that the return two days ago also significantly influences the model's predictions.\n",
    "- **`Daily_Return`**: The daily return is also a major factor, emphasizing the importance of immediate past performance in the prediction.\n",
    "- **`Lag_Return_1`**: The return one day ago is important but less so compared to the returns from two and three days ago.\n",
    "- **`Lag_3`, `Lag_2`, `Lag_1`**: The actual lagged prices for the past three days are less important than the lagged returns but still contribute to the model's predictions.\n",
    "\n",
    "### Interpretation\n",
    "- The model places more importance on lagged returns and volatility measures rather than just the raw lagged prices. This indicates that how the stock price changes over time (returns) and its volatility are more predictive of future movements than the absolute price levels of the past few days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now prepare model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_gbm_model.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_gb_model, 'final_gbm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "The final Gradient Boosting Machine (GBM) model has been trained and tuned. To see a simple sample of how to use this model, I will create a new file `deployment.ipynb` to show a simple implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stockanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
